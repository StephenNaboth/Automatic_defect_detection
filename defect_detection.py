# -*- coding: utf-8 -*-
"""defect_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dahPk03OmDyXL1Hp-Ij_7kLalgriupPV
"""

!pip install gwpy &> /dev/null
!pip install tensorflow==1.14.0 &> /dev/null
!pip install keras==2.0.8 &> /dev/null

!pip install 'h5py==2.10.0' --force-reinstall  &> /dev/null

#!git clone https://github.com/matterport/Mask_RCNN.git &> /dev/null

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/MyDrive/Mask_RCNN

#!python3 setup.py install &> /dev/
!python3 /content/drive/MyDrive/Mask_RCNN/setup.py install

!pip3 install -r /content/drive/MyDrive/Mask_RCNN/requirements.txt

!pip uninstall matplotlib

from keras.layers import Layer

# Commented out IPython magic to ensure Python compatibility.
# import basic libraries
import os
from os import listdir
import sys
import json
import datetime

# import advance libraries
from xml.etree import ElementTree
import skimage.draw
import cv2
import imgaug

# import mask rcnn libraries
from mrcnn.utils import Dataset
from mrcnn.config import Config
from mrcnn.model import MaskRCNN
from mrcnn.visualize import display_instances
from mrcnn.utils import extract_bboxes
from mrcnn.utils import compute_ap
from mrcnn.model import load_image_gt
from mrcnn.model import mold_image
from mrcnn import visualize

# import matplotlib library
import matplotlib.pyplot as plt

# import numpy libraries
import numpy as np
from numpy import zeros
from numpy import asarray
from numpy import expand_dims
from numpy import mean

# import keras libraries
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array

# ignore warnings
import warnings
warnings.filterwarnings("ignore")

# inline matplotlib
# %matplotlib inline

#!unzip /content/drive/MyDrive/Thesis_dataset/EDIT.zip -d /content/ &> /dev/null

#!unzip /content/gdrive/MyDrive/Colabnotebooks/mask_rcnn_balloon.zip -d /content/
#!unzip /content/drive/MyDrive/mask_rcnn_coco.h5.zip -d /content/ &> /dev/null

class DefectDataset(Dataset):
  def load_dataset(self, dataset_dir, subset):
    self.add_class("Defect", 1, "persistent")
    self.add_class("Defect", 2, "temporal")

    # Train or validation dataset?
    assert subset in ["train", "val"]
    dataset_dir = os.path.join(dataset_dir, subset)


    annotations = json.load(open(os.path.join(dataset_dir, "via_region_data.json")))
    annotations = list(annotations.values())  # don't need the dict keys


    #annotations = [a for a in annotations if a['regions']]

    # Add images
    for a in annotations:
        polygons = [r['shape_attributes'] for r in a['regions']]
        objects = [s['region_attributes']['Defect'] for s in a['regions']]
        
        #print(polygons)
        #print(objects)
        #num_ids=[]
        num_ids = [1 if element == 'persistent' else 2 for element in objects]
        
        image_path = os.path.join(dataset_dir, a['filename'])
        image = skimage.io.imread(image_path)
        height, width = image.shape[:2]

        self.add_image(
            "Defect",
            image_id=a['filename'],  # use file name as a unique image id
            path=image_path,
            width=width, height=height,
            polygons=polygons,
            num_ids=num_ids)


       
       
  # this function calls on the extract_boxes method and is used to load a mask for each instance in an image
  # returns a boolean mask with following dimensions width * height * instances
  def load_mask(self, image_id):
        
      # info points to the current image_id
      info = self.image_info[image_id]
        
      # for cases when source is not damage
      if info["source"] != "Defect":
          return super(self.__class__, self).load_mask(image_id)
        
      # get the class ids in an image
      num_ids = info['num_ids']
        
        
        
        # we create len(info["polygons"])(total number of polygons) number of masks of height 'h' and width 'w'
      mask = np.zeros([info["height"], info["width"], len(info["polygons"])],
                      dtype=np.uint8)
        
        
      for i, p in enumerate(info["polygons"]):
          rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])

          mask[rr, cc, i] = 1
            
        # return masks and class_ids as array
      num_ids = np.array(num_ids, dtype=np.int32)
      return mask, num_ids
    
    # this functions takes the image_id and returns the path of the image
  def image_reference(self, image_id):
      info = self.image_info[image_id]
      if info["source"] == "Defect":
          return info["path"]
      else:
          super(self.__class__, self).image_reference(image_id)

# define a configuration for the model
class DefectConfig(Config):
     # name of the configuration
    NAME = "Defect_cfg"
    
    # Adjust down if you use a smaller GPU.
    IMAGES_PER_GPU = 1

    # damage class + background class
    NUM_CLASSES = 1 + 2
    
    IMAGE_MAX_DIM = 512
    #BACKBONE = "resnet50"
    # steps per epoch and minimum confidence
    STEPS_PER_EPOCH = 100

    # Skip detections with < 90% confidence
    #DETECTION_MIN_CONFIDENCE = 0.8
    
    # learning rate and momentum
    #LEARNING_RATE=0.002
    #LEARNING_MOMENTUM = 0.8
    
    # regularization penalty
    #WEIGHT_DECAY = 0.0001
    
    # image size is controlled by this parameter
    #IMAGE_MIN_DIM = 512
    
    # validation steps
    #VALIDATION_STEPS = 50
    
    # number of Region of Interest generated per image
    #Train_ROIs_Per_Image = 200
    
    # RPN Acnhor scales and ratios to find ROI
    #RPN_ANCHOR_SCALES = (8, 16, 32, 48, 64)
    #RPN_ANCHOR_RATIOS = [0.5, 1, 1.5]

config = DefectConfig()
config.display()

# prepare train dataset.
train_set = DefectDataset()
# change the dataset 
train_set.load_dataset("/content/drive/MyDrive/Thesis_dataset/Dataset", "train")
train_set.prepare()

# prepare validation/test dataset
test_set = DefectDataset()
test_set.load_dataset("/content/drive/MyDrive/Thesis_dataset/Dataset", "val")
test_set.prepare()

# load damage config
config = DefectConfig()

# define the model
model = MaskRCNN(mode='training', model_dir='./', config=config)

# load weights mscoco model weights
weights_path = "/content/drive/MyDrive/mask_rcnn_defect_cfg_0003.h5" #'mask_rcnn_coco.h5'

# load the model weights
model.load_weights(weights_path, 
                   by_name=True, 
                   exclude=["mrcnn_class_logits", "mrcnn_bbox_fc","mrcnn_bbox", "mrcnn_mask"])

# start the training of model
# you can change epochs and layers (head or all)
model.train(train_set, 
            test_set, 
            learning_rate=config.LEARNING_RATE/10, 
            epochs=15, 
            layers='all')

# Save weights
# Typically not needed because callbacks save after every epoch
# Uncomment to save manually
model_path = os.path.join("/content/drive/MyDrive", "mask_rcnn_defect_final.h5")
model.keras_model.save_weights(model_path)

# we define a prediction configuration 
class PredictionConfig(Config):
    NAME = "Defect"
    NUM_CLASSES = 1 + 2
    DETECTION_MIN_CONFIDENCE = 0.90
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1
    IMAGE_MAX_DIM = 512
    DETECTION_MAX_INSTANCES = 3

def evaluate_model(dataset, model, cfg):
    # Interactive array to create our metrics
    APs = []
    precisions_arr = []
    recalls_arr = []
    overlaps_arr = []
    class_ids_arr = []
    scores_arr = []
    for image_id in dataset.image_ids:

        image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, cfg, image_id, use_mini_mask=False)

        scaled_image = mold_image(image, cfg)

        sample = expand_dims(scaled_image, 0)

        yhat = model.detect(sample, verbose=0)

        r = yhat[0]

        AP, precisions, recalls, overlaps = compute_ap(gt_bbox, gt_class_id, gt_mask, r["rois"], r["class_ids"], r["scores"], r['masks'])
        # Append AP to AP array
        APs.append(AP)

        # Append precisions
        for precision in precisions:
          precisions_arr.append(precision)
        
        # Append recalls
        for recall in recalls:
          recalls_arr.append(recall)

        # Append overlaps
        for overlap in overlaps:
          overlaps_arr.append(overlap)

        # Append class_ids
        for class_id in r["class_ids"]:
          class_ids_arr.append(class_id)

        # Append class_ids
        for score in r["scores"]:
          scores_arr.append(score)

      
    mAP = mean(APs)

# train dataset
train_set = DefectDataset()
train_set.load_dataset("/content/EDIT DATASET/Dataset", "train")
train_set.prepare()

# Validation dataset
test_set = DefectDataset()
test_set.load_dataset("/content/drive/MyDrive/Thesis_dataset/Dataset", "val")
test_set.prepare()

# load prediction configuration
cfg = PredictionConfig()

# define the model
model = MaskRCNN(mode='inference', model_dir='./', config=cfg)

# load model weights
# model_path = '/content/drive/MyDrive/mask_rcnn_defect_final.h5'
model_path ='/content/drive/MyDrive/Mask_RCNN/defect_cfg20220725T1949/mask_rcnn_defect_cfg_0003.h5'
model.load_weights(model_path, by_name=True)

# evaluate model on train dataset
train_mAP = evaluate_model(train_set, model, cfg)
print("Train mAP: %.3f" % train_mAP)

print(train_mAP)

# evaluate model on test dataset
test_mAP = evaluate_model(test_set, model, cfg)
print("Test mAP: %.3f" % test_mAP)

!pip install matplotlib==3.1.3

import matplotlib
import matplotlib.pyplot as plt

image = load_img("/content/drive/MyDrive/Thesis_dataset/Dataset/val/file105.png")
image = img_to_array(image)

results = model.detect([image], verbose=1)

class_names = ['BG','persistent', 'temporal']

r = results[0]

visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names,  r['scores'])

image = load_img("/content/drive/MyDrive/Thesis_dataset/Dataset/val/file401.png")
image = img_to_array(image)

results = model.detect([image], verbose=1)

class_names = ['BG','persistent', 'banana']

r = results[0]

visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names,  r['scores'])

evaluate_model(test_set, model, cfg)

visualize.plot_precision_recall(AP,precisions_arr,recalls_arr)

APs = []
    precisions_arr = []
    recalls_arr = []
    overlaps_arr = []
    class_ids_arr = []
    scores_arr = []